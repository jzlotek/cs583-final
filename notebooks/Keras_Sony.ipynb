{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_Sony.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZVOokgnh5HV",
        "colab_type": "text"
      },
      "source": [
        "# Read the README [in the repository](https://github.com/jzlotek/cs583-final) before continuing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49FnX_pjGfZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# download_dataset.py\n",
        "#\n",
        "\n",
        "import requests\n",
        "import urllib\n",
        "import os\n",
        "import sys\n",
        "from zipfile import ZipFile\n",
        "\n",
        "OUTDIR='./dataset'\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True, timeout=None)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True, timeout=None)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 10000\n",
        "    downloaded = 0\n",
        "    total = 26926678016.00 * 1.1 / 100.00\n",
        "    sys.stdout.write(\"\\r0.00% downloaded\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "                downloaded+=len(chunk)\n",
        "                if downloaded % (CHUNK_SIZE*50) == 0:\n",
        "                    sys.stdout.write('\\r%0.2f%% downloaded' % (downloaded/(total)))\n",
        "    sys.stdout.write(\"\\r100.00% downloaded\")\n",
        "\n",
        "# Ensure directory exists\n",
        "if not os.path.exists(OUTDIR):\n",
        "  os.mkdir(OUTDIR)\n",
        "\n",
        "filepath = OUTDIR+'/Sony.zip'\n",
        "# Too much data, download the smaller dataset for now\n",
        "print('Downloading Sony data... (25GB)')\n",
        "download_file_from_google_drive('10kpAcvldtcb9G2ze5hTcF1odzu4V_Zvh', filepath)\n",
        "\n",
        "fileout = OUTDIR+'/Sony'\n",
        "# Always unzip to reset directory\n",
        "print('\\nUnzipping, this will take a while...')\n",
        "print('Usage of storage should be around 117GB')\n",
        "ZipFile(filepath).extractall(path=OUTDIR)\n",
        "\n",
        "    \n",
        "print(\"Data ready in %s\" % (fileout))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGTHg8HGiYmB",
        "colab_type": "text"
      },
      "source": [
        "Data is downloaded, time to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMPKXMwObXGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras\n",
        "!pip install rawpy\n",
        "!pip install imageio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtTAfR4VtlaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# better training\n",
        "#\n",
        "\n",
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import rawpy\n",
        "\n",
        "from keras import backend as k\n",
        "from keras.layers import \\\n",
        "Input, Conv2D, LeakyReLU, MaxPooling2D, \\\n",
        "UpSampling2D, Conv2DTranspose, Concatenate, \\\n",
        "ZeroPadding2D, UpSampling3D\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import keras.layers.advanced_activations\n",
        "import keras.optimizers\n",
        "from keras.utils import plot_model\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Constants\n",
        "target_dir = './dataset/Sony/long/'\n",
        "input_dir = './dataset/Sony/short/'\n",
        "\n",
        "ps = 512 # patch size\n",
        "\n",
        "# Helper Functions\n",
        "def pack_raw(raw):\n",
        "    # pack Bayer image to 4 channels\n",
        "    im = raw.raw_image_visible.astype(np.float32)\n",
        "    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level\n",
        "\n",
        "    im = np.expand_dims(im, axis=2)\n",
        "    img_shape = im.shape\n",
        "    H = img_shape[0]\n",
        "    W = img_shape[1]\n",
        "\n",
        "    out = np.concatenate((im[0:H:2, 0:W:2, :],\n",
        "                          im[0:H:2, 1:W:2, :],\n",
        "                          im[1:H:2, 1:W:2, :],\n",
        "                          im[1:H:2, 0:W:2, :]), axis=2)\n",
        "    return out\n",
        "  \n",
        "def load_batches(batch_size):   \n",
        "    train_fns = glob.glob(target_dir + '0*.ARW') \n",
        "    train_ids = np.array([int(os.path.basename(train_fn)[0:5]) \\\n",
        "                          for train_fn in train_fns])\n",
        "    \n",
        "    train_ids = np.random.permutation(train_ids)\n",
        "    \n",
        "    total_length = len(train_ids)\n",
        "    \n",
        "    while True:\n",
        "        batch_start = 0\n",
        "        batch_end = batch_size\n",
        "\n",
        "        while batch_start < total_length:\n",
        "            limit = min(batch_end, total_length)  \n",
        "            batch_ids = train_ids[batch_start:limit]\n",
        "            \n",
        "            # Intialize return values\n",
        "            inputs, targets = np.empty((batch_size,ps,ps,4)), np.empty((batch_size,ps*2,ps*2,3))\n",
        "            index = 0\n",
        "            for image_id in batch_ids:\n",
        "              \n",
        "                # Get an random input filename\n",
        "                input_files = glob.glob(input_dir + '%05d_00*.ARW' % image_id)\n",
        "                input_path = None\n",
        "                if len(input_files) <= 1:\n",
        "                    input_path = input_files[0]\n",
        "                else :\n",
        "                    input_path = input_files[np.random.randint(0, len(input_files) - 1)]\n",
        "                input_fn = os.path.basename(input_path)\n",
        "\n",
        "                # Get the matching target name\n",
        "                target_files = glob.glob(target_dir + '%05d_00*.ARW' % image_id)\n",
        "                target_path = target_files[0]\n",
        "                target_fn = os.path.basename(target_path)\n",
        "                          \n",
        "                # Calculate amplification ratio\n",
        "                input_exposure = float(input_fn[9:-5])\n",
        "                target_exposure = float(target_fn[9:-5])\n",
        "                ratio = min(target_exposure / input_exposure, 300)\n",
        "                \n",
        "                # Load image into memory\n",
        "                raw = rawpy.imread(input_path)\n",
        "                input_image = \\\n",
        "                  np.expand_dims(pack_raw(raw), axis=0) * ratio\n",
        "\n",
        "                target_image = rawpy.imread(target_path)\n",
        "                target_image = target_image.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n",
        "                target_image = np.expand_dims(np.float32(target_image / 65535.0), axis=0)\n",
        "\n",
        "                # Take a random patch from each image\n",
        "                H = input_image.shape[1]\n",
        "                W = input_image.shape[2]\n",
        "\n",
        "                xx = np.random.randint(0, W - ps)\n",
        "                yy = np.random.randint(0, H - ps)\n",
        "                input_patch = input_image[:, yy:yy + ps, xx:xx + ps, :]\n",
        "                target_patch = target_image[:, yy * 2:yy * 2 + ps * 2, xx * 2:xx * 2 + ps * 2, :]\n",
        "        \n",
        "                # Compute random alterations to the images\n",
        "                if np.random.randint(2, size=1)[0] == 1:  # random flip\n",
        "                    input_patch = np.flip(input_patch, axis=1)\n",
        "                    target_patch = np.flip(target_patch, axis=1)\n",
        "                if np.random.randint(2, size=1)[0] == 1:\n",
        "                    input_patch = np.flip(input_patch, axis=2)\n",
        "                    target_patch = np.flip(target_patch, axis=2)\n",
        "                if np.random.randint(2, size=1)[0] == 1:  # random transpose\n",
        "                    input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n",
        "                    target_patch = np.transpose(target_patch, (0, 2, 1, 3))\n",
        "\n",
        "                input_patch = np.minimum(input_patch, 1.0)\n",
        "\n",
        "                # Add to return arrays\n",
        "                inputs[index] = input_patch\n",
        "                targets[index] = target_patch\n",
        "                \n",
        "                del input_patch\n",
        "                del target_patch\n",
        "                \n",
        "                index += 1\n",
        "\n",
        "            yield (inputs,targets,(batch_start/batch_size))\n",
        "\n",
        "            del inputs\n",
        "            del targets\n",
        "            \n",
        "            batch_start += batch_size   \n",
        "            batch_end += batch_size\n",
        "            \n",
        "        return\n",
        "\n",
        "def main():\n",
        "    # Backend Config\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
        "    k.tensorflow_backend.set_session(tf.Session(config=config))\n",
        "    k.set_image_data_format('channels_last')\n",
        "    \n",
        "    # Define the model\n",
        "    inputs = Input(shape=(None,None,4,), dtype=\"float32\",\n",
        "                name=\"Inputs\")\n",
        "\n",
        "    x = Conv2D(filters=32, kernel_size=(3,3),\n",
        "            name=\"conv_1_1\", padding='same')(inputs)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_1_act_1\")(x)\n",
        "    x = Conv2D(filters=32, kernel_size=(3,3),\n",
        "            name=\"conv_1_2\", padding='same')(x)\n",
        "    x1 = LeakyReLU(alpha=0.2, name=\"conv_1_act_2\")(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), padding='same',\n",
        "            name=\"conv_1_pool\")(x1)\n",
        "\n",
        "    x = Conv2D(filters=64, kernel_size=(3,3),\n",
        "            name=\"conv_2_1\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_2_act_1\")(x)\n",
        "    x = Conv2D(filters=64, kernel_size=(3,3),\n",
        "            name=\"conv_2_2\", padding='same')(x)\n",
        "    x2 = LeakyReLU(alpha=0.2, name=\"conv_2_act_2\")(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), padding='same',\n",
        "            name=\"conv_2_pool\")(x2)\n",
        "\n",
        "    x = Conv2D(filters=128, kernel_size=(3,3),\n",
        "            name=\"conv_3_1\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_3_act_1\")(x)\n",
        "    x = Conv2D(filters=128, kernel_size=(3,3),\n",
        "            name=\"conv_3_2\", padding='same')(x)\n",
        "    x3 = LeakyReLU(alpha=0.2, name=\"conv_3_act_2\")(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), padding='same',\n",
        "            name=\"conv_3_pool\")(x3)\n",
        "\n",
        "    x = Conv2D(filters=256, kernel_size=(3,3),\n",
        "            name=\"conv_4_1\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_4_act_1\")(x)\n",
        "    x = Conv2D(filters=256, kernel_size=(3,3),\n",
        "            name=\"conv_4_2\", padding='same')(x)\n",
        "    x4 = LeakyReLU(alpha=0.2, name=\"conv_4_act_2\")(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), padding='same',\n",
        "            name=\"conv_4_pool\")(x4)\n",
        "\n",
        "    x = Conv2D(filters=512, kernel_size=(3,3),\n",
        "            name=\"conv_5_1\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_5_act_1\")(x)\n",
        "    x = Conv2D(filters=512, kernel_size=(3,3),\n",
        "            name=\"conv_5_2\", padding='same')(x)\n",
        "    x5 = LeakyReLU(alpha=0.2, name=\"conv_5_act_2\")(x)\n",
        "    \n",
        "    x5 = Conv2DTranspose(filters=256, kernel_size=(2,2),\n",
        "            strides=(2,2), padding='same')(x5)\n",
        "    x = Concatenate(axis=3)([x5,x4])\n",
        "    x = Conv2D(filters=256, kernel_size=(3,3),\n",
        "            name=\"conv_7_1\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_7_act_1\")(x)\n",
        "    x = Conv2D(filters=256, kernel_size=(3,3),\n",
        "            name=\"conv_7_2\", padding='same')(x)\n",
        "    x6 = LeakyReLU(alpha=0.2, name=\"conv_7_act_2\")(x)\n",
        "\n",
        "    x6 = Conv2DTranspose(filters=128, kernel_size=(2,2),\n",
        "            strides=(2,2), padding='same')(x6)\n",
        "    x = Concatenate(axis=3)([x6,x3])\n",
        "    x = Conv2D(filters=128, kernel_size=(3,3),\n",
        "            name=\"conv_8_1\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_8_act_1\")(x)\n",
        "    x = Conv2D(filters=128, kernel_size=(3,3),\n",
        "            name=\"conv_8_2\", padding='same')(x)\n",
        "    x7 = LeakyReLU(alpha=0.2, name=\"conv_8_act_2\")(x)\n",
        "\n",
        "    x7 = Conv2DTranspose(filters=64, kernel_size=(2,2),\n",
        "            strides=(2,2), padding='same')(x7)\n",
        "    x = Concatenate(axis=3)([x7,x2])\n",
        "    x = Conv2D(filters=64, kernel_size=(3,3),\n",
        "            name=\"conv_9_1\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_9_act_1\")(x)\n",
        "    x = Conv2D(filters=64, kernel_size=(3,3),\n",
        "            name=\"conv_9_2\", padding='same')(x)\n",
        "    x8 = LeakyReLU(alpha=0.2, name=\"conv_9_act_2\")(x)\n",
        "\n",
        "    x8 = Conv2DTranspose(filters=32, kernel_size=(2,2),\n",
        "            strides=(2,2), padding='same')(x8)\n",
        "    x = Concatenate(axis=3)([x8,x1])\n",
        "    x = Conv2D(filters=32, kernel_size=(3,3),\n",
        "            name=\"conv_10_1\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_10_act_1\")(x)\n",
        "    x = Conv2D(filters=32, kernel_size=(3,3),\n",
        "            name=\"conv_10_2\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_10_act_2\")(x)\n",
        "\n",
        "    x = Conv2D(filters=3, kernel_size=(1,1), activation=None,\n",
        "            name=\"conv_to_image\")(x)\n",
        "\n",
        "    outputs = UpSampling2D(size=(2,2))(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name='ltsitd')\n",
        "\n",
        "    # Get info about the model\n",
        "    if False:\n",
        "        model.summary(line_length=150)\n",
        "        \n",
        "        plot_model(model, to_file='model.png')\n",
        "        from google.colab import files\n",
        "        files.download('model.png')\n",
        "    \n",
        "    batch_size = 8          # Number of images to load into memory at once (RAM/GPU Memory restricted)\n",
        "    total_epochs = 11       # Number of training rounds per batch\n",
        "\n",
        "    # Drop the learning rate by a factor of 10 halfway through\n",
        "    def step_decay(epoch):\n",
        "        lrate = 1e-4\n",
        "        \n",
        "        if epoch > total_epochs // 2:\n",
        "          lrate = 1e-5\n",
        "        \n",
        "        return lrate\n",
        "      \n",
        "    lrate = LearningRateScheduler(step_decay)\n",
        "    callback_list = [lrate]\n",
        "    \n",
        "    # Build the model\n",
        "    model.compile(loss='mae',          # mean absolute error for loss\n",
        "                  optimizer='Adam',    # Adam optimizer\n",
        "                  metrics=['acc'])     # Print accuracy info during training\n",
        "\n",
        "    # Train the model\n",
        "    for inputs, targets, batch in load_batches(batch_size):\n",
        "        print('\\n------------------------------------------------------------------------------------')\n",
        "        print(\"Training batch #%d\" % batch)\n",
        "        model.fit(inputs, \n",
        "                  targets,\n",
        "                  steps_per_epoch=batch_size,\n",
        "                  epochs=total_epochs,\n",
        "                  callbacks=callback_list, # Call the step_decay function\n",
        "                  verbose=1)\n",
        "    \n",
        "    # Save model\n",
        "    model.save('model.h5') \n",
        "    print('Saved model -> model.h5')\n",
        "    del model \n",
        "\n",
        "# Run the program\n",
        "main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_CLsAAK8QS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USsnFwAI22bz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "import imageio\n",
        "\n",
        "DIR = 'dataset/Sony/short/'\n",
        "\n",
        "INPUTS=[\n",
        "    '00182_03_0.04s.ARW',\n",
        "    '20211_01_0.033s.ARW',\n",
        "    '00186_01_0.033s.ARW',\n",
        "    '00222_05_0.04s.ARW',\n",
        "    '00209_00_0.033s.ARW',\n",
        "    '00232_03_0.04s.ARW', \n",
        "    '20201_03_0.04s.ARW',\n",
        "    '10054_01_0.04s.ARW',\n",
        "    '00207_01_0.04s.ARW',\n",
        "    '00215_00_0.033s.ARW'\n",
        "]\n",
        "\n",
        "OUT_FMT='test_%s.jpg'\n",
        "\n",
        "# Loads the saved model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "for image in INPUTS:\n",
        "    iid = image[0:5]\n",
        "    raw = rawpy.imread(DIR+image)\n",
        "\n",
        "    i = np.expand_dims(pack_raw(raw), axis=0) * 200\n",
        "    i = np.minimum(i, 1.0)\n",
        "\n",
        "    # Runs the model on a new input image\n",
        "    output = model.predict(i)\n",
        "\n",
        "    output = np.minimum(np.maximum(output, 0), 1)\n",
        "    output = output[0, :, :, :]\n",
        "\n",
        "    output *= 255\n",
        "\n",
        "    imageio.imwrite(OUT_FMT % (iid), np.array(output, dtype=np.uint8))\n",
        "    print('saved %s' % (OUT_FMT % (iid)))\n",
        "\n",
        "    files.download(DIR+image)\n",
        "    files.download(OUT_FMT % (iid))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}