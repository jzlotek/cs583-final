{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_Sony.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZVOokgnh5HV",
        "colab_type": "text"
      },
      "source": [
        "# Read the README [in the repository](https://github.com/jzlotek/cs583-final) before continuing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49FnX_pjGfZR",
        "colab_type": "code",
        "outputId": "25308247-1026-49a3-a60f-6ef0f04dd676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#\n",
        "# download_dataset.py\n",
        "#\n",
        "\n",
        "import requests\n",
        "import urllib\n",
        "import os\n",
        "import sys\n",
        "from zipfile import ZipFile\n",
        "\n",
        "OUTDIR='./dataset'\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True, timeout=None)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True, timeout=None)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 10000\n",
        "    downloaded = 0\n",
        "    total = 26926678016.00 * 1.1 / 100.00\n",
        "    sys.stdout.write(\"\\r0.00% downloaded\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "                downloaded+=len(chunk)\n",
        "                if downloaded % (CHUNK_SIZE*50) == 0:\n",
        "                    sys.stdout.write('\\r%0.2f%% downloaded' % (downloaded/(total)))\n",
        "    sys.stdout.write(\"\\r100.00% downloaded\")\n",
        "\n",
        "# Ensure directory exists\n",
        "if not os.path.exists(OUTDIR):\n",
        "  os.mkdir(OUTDIR)\n",
        "\n",
        "filepath = OUTDIR+'/Sony.zip'\n",
        "# Too much data, download the smaller dataset for now\n",
        "print('Downloading Sony data... (25GB)')\n",
        "download_file_from_google_drive('10kpAcvldtcb9G2ze5hTcF1odzu4V_Zvh', filepath)\n",
        "\n",
        "fileout = OUTDIR+'/Sony'\n",
        "# Always unzip to reset directory\n",
        "print('\\nUnzipping, this will take a while...')\n",
        "print('Usage of storage should be around 117GB')\n",
        "ZipFile(filepath).extractall(path=OUTDIR)\n",
        "\n",
        "    \n",
        "print(\"Data ready in %s\" % (fileout))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Sony data... (25GB)\n",
            "100.00% downloaded\n",
            "Unzipping, this will take a while...\n",
            "Usage of storage should be around 117GB\n",
            "Data ready in ./dataset/Sony\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGTHg8HGiYmB",
        "colab_type": "text"
      },
      "source": [
        "Data is downloaded, time to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMPKXMwObXGW",
        "colab_type": "code",
        "outputId": "d300e941-62af-47cc-d8ba-d2320e048d37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!pip install keras\n",
        "!pip install rawpy\n",
        "!pip install imageio"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.9)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.16.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.0)\n",
            "Requirement already satisfied: rawpy in /usr/local/lib/python3.6/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rawpy) (1.16.4)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (4.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.16.4)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->imageio) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtTAfR4VtlaP",
        "colab_type": "code",
        "outputId": "20872d4a-5321-4ded-88b9-02011573dac6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8959
        }
      },
      "source": [
        "#\n",
        "# better training\n",
        "#\n",
        "\n",
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import rawpy\n",
        "\n",
        "from keras import backend as k\n",
        "from keras.layers import \\\n",
        "Input, Conv2D, LeakyReLU, MaxPooling2D, \\\n",
        "UpSampling2D, Conv2DTranspose, Concatenate, \\\n",
        "ZeroPadding2D, UpSampling3D\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import keras.layers.advanced_activations\n",
        "import keras.optimizers\n",
        "from keras.utils import plot_model\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Constants\n",
        "target_dir = './dataset/Sony/long/'\n",
        "input_dir = './dataset/Sony/short/'\n",
        "\n",
        "ps = 512 # patch size\n",
        "\n",
        "# Helper Functions\n",
        "def pack_raw(raw):\n",
        "    # pack Bayer image to 4 channels\n",
        "    im = raw.raw_image_visible.astype(np.float32)\n",
        "    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level\n",
        "\n",
        "    im = np.expand_dims(im, axis=2)\n",
        "    img_shape = im.shape\n",
        "    H = img_shape[0]\n",
        "    W = img_shape[1]\n",
        "\n",
        "    out = np.concatenate((im[0:H:2, 0:W:2, :],\n",
        "                          im[0:H:2, 1:W:2, :],\n",
        "                          im[1:H:2, 1:W:2, :],\n",
        "                          im[1:H:2, 0:W:2, :]), axis=2)\n",
        "    return out\n",
        "  \n",
        "def load_batches(batch_size):   \n",
        "    train_fns = glob.glob(target_dir + '0*.ARW') \n",
        "    train_ids = np.array([int(os.path.basename(train_fn)[0:5]) \\\n",
        "                          for train_fn in train_fns])\n",
        "    \n",
        "    train_ids = np.random.permutation(train_ids)\n",
        "    \n",
        "    total_length = len(train_ids)\n",
        "    \n",
        "    while True:\n",
        "        batch_start = 0\n",
        "        batch_end = batch_size\n",
        "\n",
        "        while batch_start < total_length:\n",
        "            limit = min(batch_end, total_length)  \n",
        "            batch_ids = train_ids[batch_start:limit]\n",
        "            \n",
        "            # Intialize return values\n",
        "            inputs, targets = np.empty((batch_size,ps,ps,4)), np.empty((batch_size,ps*2,ps*2,3))\n",
        "            index = 0\n",
        "            for image_id in batch_ids:\n",
        "              \n",
        "                # Get an random input filename\n",
        "                input_files = glob.glob(input_dir + '%05d_00*.ARW' % image_id)\n",
        "                input_path = None\n",
        "                if len(input_files) <= 1:\n",
        "                    input_path = input_files[0]\n",
        "                else :\n",
        "                    input_path = input_files[np.random.randint(0, len(input_files) - 1)]\n",
        "                input_fn = os.path.basename(input_path)\n",
        "\n",
        "                # Get the matching target name\n",
        "                target_files = glob.glob(target_dir + '%05d_00*.ARW' % image_id)\n",
        "                target_path = target_files[0]\n",
        "                target_fn = os.path.basename(target_path)\n",
        "                          \n",
        "                # Calculate amplification ratio\n",
        "                input_exposure = float(input_fn[9:-5])\n",
        "                target_exposure = float(target_fn[9:-5])\n",
        "                ratio = min(target_exposure / input_exposure, 300)\n",
        "                \n",
        "                # Load image into memory\n",
        "                raw = rawpy.imread(input_path)\n",
        "                input_image = \\\n",
        "                  np.expand_dims(pack_raw(raw), axis=0) * ratio\n",
        "\n",
        "                target_image = rawpy.imread(target_path)\n",
        "                target_image = target_image.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n",
        "                target_image = np.expand_dims(np.float32(target_image / 65535.0), axis=0)\n",
        "\n",
        "                # Take a random patch from each image\n",
        "                H = input_image.shape[1]\n",
        "                W = input_image.shape[2]\n",
        "\n",
        "                xx = np.random.randint(0, W - ps)\n",
        "                yy = np.random.randint(0, H - ps)\n",
        "                input_patch = input_image[:, yy:yy + ps, xx:xx + ps, :]\n",
        "                target_patch = target_image[:, yy * 2:yy * 2 + ps * 2, xx * 2:xx * 2 + ps * 2, :]\n",
        "        \n",
        "                # Compute random alterations to the images\n",
        "                if np.random.randint(2, size=1)[0] == 1:  # random flip\n",
        "                    input_patch = np.flip(input_patch, axis=1)\n",
        "                    target_patch = np.flip(target_patch, axis=1)\n",
        "                if np.random.randint(2, size=1)[0] == 1:\n",
        "                    input_patch = np.flip(input_patch, axis=2)\n",
        "                    target_patch = np.flip(target_patch, axis=2)\n",
        "                if np.random.randint(2, size=1)[0] == 1:  # random transpose\n",
        "                    input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n",
        "                    target_patch = np.transpose(target_patch, (0, 2, 1, 3))\n",
        "\n",
        "                input_patch = np.minimum(input_patch, 1.0)\n",
        "\n",
        "                # Add to return arrays\n",
        "                inputs[index] = input_patch\n",
        "                targets[index] = target_patch\n",
        "                \n",
        "                del input_patch\n",
        "                del target_patch\n",
        "                \n",
        "                index += 1\n",
        "\n",
        "            yield (inputs,targets,(batch_start/batch_size))\n",
        "\n",
        "            del inputs\n",
        "            del targets\n",
        "            \n",
        "            batch_start += batch_size   \n",
        "            batch_end += batch_size\n",
        "            \n",
        "        return\n",
        "\n",
        "def main():\n",
        "    # Backend Config\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
        "    k.tensorflow_backend.set_session(tf.Session(config=config))\n",
        "    k.set_image_data_format('channels_last')\n",
        "    \n",
        "    # Define the model\n",
        "    inputs = Input(shape=(None,None,4,), dtype=\"float32\",\n",
        "                name=\"Inputs\")\n",
        "\n",
        "    x = Conv2D(filters=32, kernel_size=(3,3),\n",
        "            name=\"conv_1_1\", padding='same')(inputs)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_1_act_1\")(x)\n",
        "    x = Conv2D(filters=32, kernel_size=(3,3),\n",
        "            name=\"conv_1_2\", padding='same')(x)\n",
        "    x1 = LeakyReLU(alpha=0.2, name=\"conv_1_act_2\")(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), padding='same',\n",
        "            name=\"conv_1_pool\")(x1)\n",
        "\n",
        "    x = Conv2D(filters=64, kernel_size=(3,3),\n",
        "            name=\"conv_2_1\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_2_act_1\")(x)\n",
        "    x = Conv2D(filters=64, kernel_size=(3,3),\n",
        "            name=\"conv_2_2\", padding='same')(x)\n",
        "    x2 = LeakyReLU(alpha=0.2, name=\"conv_2_act_2\")(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), padding='same',\n",
        "            name=\"conv_2_pool\")(x2)\n",
        "\n",
        "    x = Conv2D(filters=128, kernel_size=(3,3),\n",
        "            name=\"conv_3_1\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_3_act_1\")(x)\n",
        "    x = Conv2D(filters=128, kernel_size=(3,3),\n",
        "            name=\"conv_3_2\", padding='same')(x)\n",
        "    x3 = LeakyReLU(alpha=0.2, name=\"conv_3_act_2\")(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), padding='same',\n",
        "            name=\"conv_3_pool\")(x3)\n",
        "\n",
        "    x = Conv2D(filters=256, kernel_size=(3,3),\n",
        "            name=\"conv_4_1\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_4_act_1\")(x)\n",
        "    x = Conv2D(filters=256, kernel_size=(3,3),\n",
        "            name=\"conv_4_2\", padding='same')(x)\n",
        "    x4 = LeakyReLU(alpha=0.2, name=\"conv_4_act_2\")(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), padding='same',\n",
        "            name=\"conv_4_pool\")(x4)\n",
        "\n",
        "    x = Conv2D(filters=512, kernel_size=(3,3),\n",
        "            name=\"conv_5_1\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_5_act_1\")(x)\n",
        "    x = Conv2D(filters=512, kernel_size=(3,3),\n",
        "            name=\"conv_5_2\", padding='same')(x)\n",
        "    x5 = LeakyReLU(alpha=0.2, name=\"conv_5_act_2\")(x)\n",
        "    \n",
        "    x5 = Conv2DTranspose(filters=256, kernel_size=(2,2),\n",
        "            strides=(2,2), padding='same')(x5)\n",
        "    x = Concatenate(axis=3)([x5,x4])\n",
        "    x = Conv2D(filters=256, kernel_size=(3,3),\n",
        "            name=\"conv_7_1\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_7_act_1\")(x)\n",
        "    x = Conv2D(filters=256, kernel_size=(3,3),\n",
        "            name=\"conv_7_2\", padding='same')(x)\n",
        "    x6 = LeakyReLU(alpha=0.2, name=\"conv_7_act_2\")(x)\n",
        "\n",
        "    x6 = Conv2DTranspose(filters=128, kernel_size=(2,2),\n",
        "            strides=(2,2), padding='same')(x6)\n",
        "    x = Concatenate(axis=3)([x6,x3])\n",
        "    x = Conv2D(filters=128, kernel_size=(3,3),\n",
        "            name=\"conv_8_1\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_8_act_1\")(x)\n",
        "    x = Conv2D(filters=128, kernel_size=(3,3),\n",
        "            name=\"conv_8_2\", padding='same')(x)\n",
        "    x7 = LeakyReLU(alpha=0.2, name=\"conv_8_act_2\")(x)\n",
        "\n",
        "    x7 = Conv2DTranspose(filters=64, kernel_size=(2,2),\n",
        "            strides=(2,2), padding='same')(x7)\n",
        "    x = Concatenate(axis=3)([x7,x2])\n",
        "    x = Conv2D(filters=64, kernel_size=(3,3),\n",
        "            name=\"conv_9_1\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_9_act_1\")(x)\n",
        "    x = Conv2D(filters=64, kernel_size=(3,3),\n",
        "            name=\"conv_9_2\", padding='same')(x)\n",
        "    x8 = LeakyReLU(alpha=0.2, name=\"conv_9_act_2\")(x)\n",
        "\n",
        "    x8 = Conv2DTranspose(filters=32, kernel_size=(2,2),\n",
        "            strides=(2,2), padding='same')(x8)\n",
        "    x = Concatenate(axis=3)([x8,x1])\n",
        "    x = Conv2D(filters=32, kernel_size=(3,3),\n",
        "            name=\"conv_10_1\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_10_act_1\")(x)\n",
        "    x = Conv2D(filters=32, kernel_size=(3,3),\n",
        "            name=\"conv_10_2\", padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2, name=\"conv_10_act_2\")(x)\n",
        "\n",
        "    x = Conv2D(filters=3, kernel_size=(1,1), activation=None,\n",
        "            name=\"conv_to_image\")(x)\n",
        "\n",
        "    outputs = UpSampling2D(size=(2,2))(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name='ltsitd')\n",
        "\n",
        "    # Get info about the model\n",
        "    if False:\n",
        "        model.summary(line_length=150)\n",
        "        \n",
        "        plot_model(model, to_file='model.png')\n",
        "        from google.colab import files\n",
        "        files.download('model.png')\n",
        "    \n",
        "    batch_size = 8          # Number of images to load into memory at once (RAM/GPU Memory restricted)\n",
        "    total_epochs = 11       # Number of training rounds per batch\n",
        "\n",
        "    # Drop the learning rate by a factor of 10 halfway through\n",
        "    def step_decay(epoch):\n",
        "        lrate = 1e-4\n",
        "        \n",
        "        if epoch > total_epochs // 2:\n",
        "          lrate = 1e-5\n",
        "        \n",
        "        return lrate\n",
        "      \n",
        "    lrate = LearningRateScheduler(step_decay)\n",
        "    callback_list = [lrate]\n",
        "    \n",
        "    # Build the model\n",
        "    model.compile(loss='mae',          # mean absolute error for loss\n",
        "                  optimizer='Adam',    # Adam optimizer\n",
        "                  metrics=['acc'])     # Print accuracy info during training\n",
        "\n",
        "    # Train the model\n",
        "    for inputs, targets, batch in load_batches(batch_size):\n",
        "        print('\\n------------------------------------------------------------------------------------')\n",
        "        print(\"Training batch #%d\" % batch)\n",
        "        model.fit(inputs, \n",
        "                  targets,\n",
        "                  steps_per_epoch=batch_size,\n",
        "                  epochs=total_epochs,\n",
        "                  callbacks=callback_list, # Call the step_decay function\n",
        "                  verbose=1)\n",
        "    \n",
        "    # Save model\n",
        "    model.save('model.h5') \n",
        "    print('Saved model -> model.h5')\n",
        "    del model \n",
        "\n",
        "# Run the program\n",
        "main()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #0\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 31s 4s/step - loss: 0.2402 - acc: 0.1877\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.2175 - acc: 0.1544\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.1616 - acc: 0.1542\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.1036 - acc: 0.1546\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0631 - acc: 0.4526\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0551 - acc: 0.6487\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0534 - acc: 0.5155\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0523 - acc: 0.5798\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0517 - acc: 0.6453\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0513 - acc: 0.6614\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0510 - acc: 0.6633\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #1\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.1319 - acc: 0.6275\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.1178 - acc: 0.6085\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.1031 - acc: 0.5714\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0936 - acc: 0.5081\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0838 - acc: 0.6205\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0762 - acc: 0.6517\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0727 - acc: 0.6529\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0721 - acc: 0.6528\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0714 - acc: 0.6521\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0707 - acc: 0.6511\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0699 - acc: 0.6506\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #2\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0615 - acc: 0.7886\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0557 - acc: 0.7951\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0525 - acc: 0.7950\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0507 - acc: 0.7965\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0491 - acc: 0.7949\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0475 - acc: 0.7964\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0464 - acc: 0.7972\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0461 - acc: 0.7971\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0459 - acc: 0.7970\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0457 - acc: 0.7967\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0455 - acc: 0.7965\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #3\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0781 - acc: 0.7287\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0694 - acc: 0.6795\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0643 - acc: 0.7198\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0603 - acc: 0.6848\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0575 - acc: 0.6760\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0551 - acc: 0.6840\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0551 - acc: 0.6760\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0539 - acc: 0.6788\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0535 - acc: 0.6880\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0532 - acc: 0.6826\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0530 - acc: 0.6817\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #4\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0492 - acc: 0.7450\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0451 - acc: 0.7778\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0433 - acc: 0.7844\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0420 - acc: 0.7850\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0409 - acc: 0.7930\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0400 - acc: 0.7961\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0394 - acc: 0.7976\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0393 - acc: 0.7982\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0392 - acc: 0.7976\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0390 - acc: 0.7975\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0389 - acc: 0.7975\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #5\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0730 - acc: 0.6542\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0711 - acc: 0.6506\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0634 - acc: 0.6587\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0590 - acc: 0.6624\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0569 - acc: 0.6613\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0555 - acc: 0.6609\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0546 - acc: 0.6628\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0545 - acc: 0.6621\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0544 - acc: 0.6641\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0542 - acc: 0.6643\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0541 - acc: 0.6645\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #6\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0458 - acc: 0.7408\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0413 - acc: 0.7592\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0387 - acc: 0.8023\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0375 - acc: 0.8185\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0365 - acc: 0.8181\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0369 - acc: 0.8158\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0363 - acc: 0.8205\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0357 - acc: 0.8225\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0356 - acc: 0.8218\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0354 - acc: 0.8215\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0354 - acc: 0.8221\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #7\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0422 - acc: 0.6184\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0403 - acc: 0.6208\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0394 - acc: 0.6185\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0387 - acc: 0.6196\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0380 - acc: 0.6273\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0374 - acc: 0.6314\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0371 - acc: 0.6336\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0370 - acc: 0.6342\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0369 - acc: 0.6348\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0368 - acc: 0.6357\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0368 - acc: 0.6364\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #8\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0599 - acc: 0.6827\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0557 - acc: 0.6854\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0536 - acc: 0.6940\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0524 - acc: 0.6944\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0515 - acc: 0.6943\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0508 - acc: 0.6921\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0501 - acc: 0.6939\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0500 - acc: 0.6916\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0499 - acc: 0.6911\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0498 - acc: 0.6914\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0497 - acc: 0.6923\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #9\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0458 - acc: 0.5860\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0423 - acc: 0.6393\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0403 - acc: 0.6507\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0392 - acc: 0.6697\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0384 - acc: 0.6771\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0380 - acc: 0.6846\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0371 - acc: 0.6882\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0370 - acc: 0.6929\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0369 - acc: 0.6921\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0368 - acc: 0.6922\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0368 - acc: 0.6935\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #10\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0598 - acc: 0.6488\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0466 - acc: 0.6197\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0394 - acc: 0.6712\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0363 - acc: 0.7047\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0346 - acc: 0.7140\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0334 - acc: 0.7244\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0329 - acc: 0.7291\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0328 - acc: 0.7323\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0327 - acc: 0.7316\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0326 - acc: 0.7321\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0325 - acc: 0.7343\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #11\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0490 - acc: 0.7816\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0434 - acc: 0.7946\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0402 - acc: 0.7859\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0389 - acc: 0.8111\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0380 - acc: 0.8177\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0372 - acc: 0.8172\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0369 - acc: 0.8227\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0368 - acc: 0.8216\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0367 - acc: 0.8220\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0367 - acc: 0.8210\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0366 - acc: 0.8212\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #12\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0462 - acc: 0.6319\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0397 - acc: 0.6785\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0372 - acc: 0.6828\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0356 - acc: 0.6744\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0345 - acc: 0.6751\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0335 - acc: 0.6812\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0330 - acc: 0.6854\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0329 - acc: 0.6855\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0328 - acc: 0.6859\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0328 - acc: 0.6869\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0327 - acc: 0.6879\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #13\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0411 - acc: 0.7544\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0374 - acc: 0.7328\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0350 - acc: 0.7743\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0343 - acc: 0.7746\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0332 - acc: 0.7778\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0323 - acc: 0.7793\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0318 - acc: 0.7795\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0317 - acc: 0.7802\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0316 - acc: 0.7805\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0315 - acc: 0.7808\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0314 - acc: 0.7810\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #14\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0389 - acc: 0.6593\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0336 - acc: 0.6695\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0306 - acc: 0.6585\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0281 - acc: 0.6895\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0266 - acc: 0.6716\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0270 - acc: 0.6940\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0259 - acc: 0.7048\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0256 - acc: 0.7105\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0253 - acc: 0.7119\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0252 - acc: 0.7118\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0251 - acc: 0.7111\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #15\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0545 - acc: 0.4668\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0451 - acc: 0.5511\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0423 - acc: 0.5393\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0408 - acc: 0.5762\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0395 - acc: 0.5904\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0387 - acc: 0.5947\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0382 - acc: 0.6026\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0381 - acc: 0.5955\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0380 - acc: 0.5931\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0380 - acc: 0.5943\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0379 - acc: 0.5928\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #16\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0436 - acc: 0.7258\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0397 - acc: 0.7361\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0370 - acc: 0.7663\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0354 - acc: 0.7468\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0343 - acc: 0.7282\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0336 - acc: 0.7282\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0333 - acc: 0.7285\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0332 - acc: 0.7351\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0331 - acc: 0.7347\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0330 - acc: 0.7321\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0330 - acc: 0.7323\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #17\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0471 - acc: 0.6544\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0441 - acc: 0.7120\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0423 - acc: 0.7276\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0412 - acc: 0.7243\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0404 - acc: 0.7389\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0397 - acc: 0.7386\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0394 - acc: 0.7455\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0393 - acc: 0.7464\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0393 - acc: 0.7470\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0392 - acc: 0.7472\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0392 - acc: 0.7479\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #18\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0355 - acc: 0.6896\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0331 - acc: 0.6916\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0323 - acc: 0.7345\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0317 - acc: 0.7211\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0312 - acc: 0.7320\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0309 - acc: 0.7311\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0307 - acc: 0.7330\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0307 - acc: 0.7334\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0306 - acc: 0.7334\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0306 - acc: 0.7338\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.0306 - acc: 0.7338\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #19\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0432 - acc: 0.6036\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0400 - acc: 0.6610\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0380 - acc: 0.6687\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0369 - acc: 0.6853\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 7s 914ms/step - loss: 0.0372 - acc: 0.6776\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 7s 937ms/step - loss: 0.0363 - acc: 0.6821\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 8s 943ms/step - loss: 0.0354 - acc: 0.7059\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 8s 945ms/step - loss: 0.0352 - acc: 0.7031\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 7s 932ms/step - loss: 0.0351 - acc: 0.7043\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 7s 931ms/step - loss: 0.0350 - acc: 0.7084\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 8s 949ms/step - loss: 0.0349 - acc: 0.7083\n",
            "\n",
            "------------------------------------------------------------------------------------\n",
            "Training batch #20\n",
            "Epoch 1/11\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.0519 - acc: 0.5326\n",
            "Epoch 2/11\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.0468 - acc: 0.6954\n",
            "Epoch 3/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0441 - acc: 0.5193\n",
            "Epoch 4/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0412 - acc: 0.6572\n",
            "Epoch 5/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0381 - acc: 0.6022\n",
            "Epoch 6/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0351 - acc: 0.6219\n",
            "Epoch 7/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0335 - acc: 0.5671\n",
            "Epoch 8/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0331 - acc: 0.6176\n",
            "Epoch 9/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0327 - acc: 0.6276\n",
            "Epoch 10/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0323 - acc: 0.6107\n",
            "Epoch 11/11\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.0319 - acc: 0.6183\n",
            "Saved model -> model.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_CLsAAK8QS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USsnFwAI22bz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "import imageio\n",
        "\n",
        "DIR = 'dataset/Sony/short/'\n",
        "\n",
        "INPUTS=[\n",
        "    '00182_03_0.04s.ARW',\n",
        "    '20211_01_0.033s.ARW',\n",
        "    '00186_01_0.033s.ARW',\n",
        "    '00222_05_0.04s.ARW',\n",
        "    '00209_00_0.033s.ARW',\n",
        "    '00232_03_0.04s.ARW', \n",
        "    '20201_03_0.04s.ARW',\n",
        "    '10054_01_0.04s.ARW',\n",
        "    '00207_01_0.04s.ARW',\n",
        "    '00215_00_0.033s.ARW'\n",
        "]\n",
        "\n",
        "OUT_FMT='test_%s.jpg'\n",
        "\n",
        "# Loads the saved model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "for image in INPUTS:\n",
        "    iid = image[0:5]\n",
        "    raw = rawpy.imread(DIR+image)\n",
        "\n",
        "    i = np.expand_dims(pack_raw(raw), axis=0) * 200\n",
        "    i = np.minimum(i, 1.0)\n",
        "\n",
        "    # Runs the model on a new input image\n",
        "    output = model.predict(i)\n",
        "\n",
        "    output = np.minimum(np.maximum(output, 0), 1)\n",
        "    output = output[0, :, :, :]\n",
        "\n",
        "    output *= 255\n",
        "\n",
        "    imageio.imwrite(OUT_FMT % (iid), np.array(output, dtype=np.uint8))\n",
        "    print('saved %s' % (OUT_FMT % (iid)))\n",
        "\n",
        "    files.download(DIR+image)\n",
        "    files.download(OUT_FMT % (iid))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}